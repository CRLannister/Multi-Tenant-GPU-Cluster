cull:
  enabled: true
  every: 900
  timeout: 14400
hub:
  config:
    JupyterHub:
      admin_access: true
      admin_users:
      - aagar8
      - user1
      allow_named_servers: false
      authenticator_class: profileaccessauthenticator.ProfileFirstUseAuth
      debug: true
      log_format: '%(asctime)s %(levelname)s %(message)s'
      log_level: DEBUG
      open_signup: false
      password_change_allowed: true
      signup_allowed: false
  db:
    pvc:
      accessModes:
      - ReadWriteMany
      storage: 5Gi
      storageClassName: nfs-client
    type: sqlite-pvc
  extraConfig:
    00-auth: |
      import sys
      sys.path.append('/etc/jupyterhub')
      from profile_access_authenticator import ProfileFirstUseAuth
      from traitlets import Dict, Unicode, List
      c.JupyterHub.authenticator_class = ProfileFirstUseAuth  # Override with our custom class
      c.ProfileFirstUseAuth.whitelist = set(['aagar8', 'user1', 'user2', 'test'])  # Match with your user_profile_access

      # Define profile access
      c.ProfileFirstUseAuth.user_profile_access = {
          'test': ["Lambda Tiny GPU"],
          'user1': ["No GPU - Tiny", "Lambda Tiny GPU", 'A100 - Dual GPU Distributed'],
          'user2': ["No GPU - Tiny", "Lambda Tiny GPU", 'Lambda - Dual GPU Distributed'],
          'aagar8': ['No GPU - Tiny', 'No GPU - Small', 'Lambda Tiny GPU', 'A100 Single Large GPU', 'Lambda - Dual GPU Distributed', 'A100 - Dual GPU Distributed', 'A100 - Quadro GPU Distributed', 'HPC - Maximum Compute']
      }
    01-profiles: |
      # Profile list configuration
      c.KubeSpawner.profile_list = [
          {
              'display_name': 'No GPU - Tiny',
              'description': 'Minimal resources for light workloads, (C2-M8-G0)',
              'kubespawner_override': {
                  'cpu_guarantee': 1,
                  'cpu_limit': 2,
                  'mem_guarantee': '4G',
                  'mem_limit': '8G',
                  'storage_capacity': '200Gi',
                  'extra_resource_limits': {'nvidia.com/gpu': '0'},
                  'node_selector': {'gpu-type': 'lambda-10gb'},
                  'tolerations': [
                      {'key': 'gpu-type', 'operator': 'Equal', 'value': 'lambda-10gb', 'effect': 'NoSchedule'}
                  ]
              }
          },
          {
              'display_name': 'No GPU - Small',
              'description': 'Light compute resources, no GPU, (C4-M16-G0)',
              'kubespawner_override': {
                  'cpu_guarantee': 2,
                  'cpu_limit': 4,
                  'mem_guarantee': '8G',
                  'mem_limit': '16G',
                  'storage_capacity': '200Gi',
                  'extra_resource_limits': {'nvidia.com/gpu': '0'},
                  'node_selector': {'gpu-type': 'lambda-10gb'},
                  'tolerations': [
                      {'key': 'gpu-type', 'operator': 'Equal', 'value': 'lambda-10gb', 'effect': 'NoSchedule'}
                  ]
              }
          },
          {
              'display_name': 'Lambda Tiny GPU',
              'description': '10GB per GPU, 1 GPU for small GPU tasks, (C8-M32-G1@10)',
              'kubespawner_override': {
                  'cpu_guarantee': 4,
                  'cpu_limit': 8,
                  'mem_guarantee': '16G',
                  'mem_limit': '32G',
                  'storage_capacity': '200Gi',
                  'extra_resource_limits': {'nvidia.com/gpu': '1'},
                  'node_selector': {'gpu-type': 'lambda-10gb'},
                  'tolerations': [
                      {'key': 'gpu-type', 'operator': 'Equal', 'value': 'lambda-10gb', 'effect': 'NoSchedule'}
                  ]
              }
          },
          {
              'display_name': 'A100 Single Large GPU',
              'description': '80GB A100 GPU for large-scale GPU tasks, (C8-M128-G1@80)',
              'kubespawner_override': {
                  'cpu_guarantee': 8,
                  'cpu_limit': 8,
                  'mem_guarantee': '64G',
                  'mem_limit': '128G',
                  'storage_capacity': '200Gi',
                  'extra_resource_limits': {'nvidia.com/gpu': '1'}
              }
          },
          {
              'display_name': 'Lambda - Dual GPU Distributed',
              'description': '2 Lambda GPUs for distributed training, (C16-M32-G2@10)',
              'kubespawner_override': {
                  'cpu_guarantee': 8,
                  'cpu_limit': 16,
                  'mem_guarantee': '32G',
                  'mem_limit': '32G',
                  'storage_capacity': '200Gi',
                  'extra_resource_limits': {'nvidia.com/gpu': '2'},
                  'node_selector': {'gpu-type': 'lambda-10gb'},
                  'tolerations': [
                      {'key': 'gpu-type', 'operator': 'Equal', 'value': 'lambda-10gb', 'effect': 'NoSchedule'}
                  ]
              }
          },
          {
              'display_name': 'A100 - Dual GPU Distributed',
              'description': '2 A100 GPUs for large-scale distributed training, (C8-M256-G2@80)',
              'kubespawner_override': {
                  'cpu_guarantee': 8,
                  'cpu_limit': 8,
                  'mem_guarantee': '128G',
                  'mem_limit': '256G',
                  'storage_capacity': '200Gi',
                  'extra_resource_limits': {'nvidia.com/gpu': '2'}
              }
          },
          {
              'display_name': 'A100 - Quadro GPU Distributed',
              'description': 'Mixed GPU configuration for complex ML research, (C12-M256-G4@80)',
              'kubespawner_override': {
                  'cpu_guarantee': 12,
                  'cpu_limit': 12,
                  'mem_guarantee': '256G',
                  'mem_limit': '256G',
                  'storage_capacity': '200Gi',
                  'extra_resource_limits': {'nvidia.com/gpu': '4'}
              }
          },
          {
              'display_name': 'HPC - Maximum Compute',
              'description': 'Maximum resources for high-performance computing, (C16-M512-G8@80)',
              'kubespawner_override': {
                  'cpu_guarantee': 12,
                  'cpu_limit': 16,
                  'mem_guarantee': '256G',
                  'mem_limit': '512G',
                  'storage_capacity': '200Gi',
                  'extra_resource_limits': {'nvidia.com/gpu': '8'}
              }
          }
      ]
    debugging: |
      c.JupyterHub.log_level = 'DEBUG'
      c.Spawner.debug = True
    logging: |
      import logging
      c.JupyterHub.log_level = 'DEBUG'
      # Configure root logger
      logging.basicConfig(level=logging.DEBUG)
      # Configure jupyterhub logger
      logger = logging.getLogger('jupyterhub')
      logger.setLevel(logging.DEBUG)
  extraFiles:
    profile_access_authenticator.py:
      mountPath: /etc/jupyterhub/profile_access_authenticator.py
      stringData: "from firstuseauthenticator import FirstUseAuthenticator\nfrom traitlets
        import Dict, Unicode, List\nimport json\nimport logging\nlogger = logging.getLogger('jupyterhub')\n\nclass
        ProfileFirstUseAuth(FirstUseAuthenticator):\n    \"\"\"\n    Extends FirstUseAuthenticator
        to:\n    1. Inherit all authentication and template handling\n    2. Add profile
        access control\n    \"\"\"\n    # Disable new user signup\n    whitelist =
        {'aagar8', 'user1', 'user2', 'test'}  # Add all allowed users\n\n    def authenticate(self,
        handler, data):\n        username = data['username']\n        if username
        not in self.whitelist:\n            self.log.warning(f\"Unauthorized signup
        attempt: {username}\")\n            return None\n        if username not in
        self.user_profile_access:\n            self.log.warning(f\"User not in profile
        access list: {username}\")\n            return None\n        return super().authenticate(handler,
        data)\n         \n    # Define the configuration trait for profile access\n
        \   user_profile_access = Dict(\n        key_trait=Unicode(),\n        value_trait=List(Unicode()),\n
        \       default_value={}\n    ).tag(config=True)\n\n    def pre_spawn_start(self,
        user, spawner):\n        \"\"\"\n        Filter available profiles before
        spawn\n        Only show profiles the user has access to\n        \"\"\"\n
        \       logger.debug(f\"Checking profiles for user: {user.name}\")\n        logger.debug(f\"Current
        profile list: {spawner.profile_list}\")\n        logger.debug(f\"User profile
        access dict: {self.user_profile_access}\")\n        allowed_profiles = self.user_profile_access.get(user.name,
        [])\n        logger.debug(f\"Allowed profiles for {user.name}: {allowed_profiles}\")\n
        \       if not allowed_profiles:\n            spawner.profile_list = []\n
        \           return\n\n        original_list = getattr(spawner, 'profile_list',
        [])\n        logger.debug(f\"original_list: {original_list}\")\n        spawner.profile_list
        = [\n            profile for profile in original_list\n            if profile.get('display_name')
        in allowed_profiles\n        ]\n        logger.debug(f\"Filtered profile list:
        {spawner.profile_list}\")\n\n        return super().pre_spawn_start(user,
        spawner)\n"
ingress:
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/upstream-port: "8000"
  enabled: true
  hosts:
  - jupyterhub2.local
  - jupyter.sail
  - null
  ingressClassName: nginx
proxy:
  https:
    enabled: false
  service:
    type: ClusterIP
singleuser:
  extraResource:
    guarantees:
      cpu: "2"
      memory: 8Gi
    limits:
      cpu: "8"
      memory: 16Gi
  image:
    name: jupyter/datascience-notebook
    tag: latest
  storage:
    capacity: 200Gi
    dynamic:
      storageClass: nfs-client
    type: dynamic
